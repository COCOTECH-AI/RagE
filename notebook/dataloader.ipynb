{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python310\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd \n",
    "import rag_chatbot\n",
    "from rag_chatbot import SentABCollate, SentABDL \n",
    "from rag_chatbot.utils.augment_text import NoiseDropout\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.5\n"
     ]
    }
   ],
   "source": [
    "print(rag_chatbot.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label_is_numerical'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL # support binary,categorical, mselog, cosine_similarity loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_sts_df= pd.read_json('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x_1': {'input_ids': tensor([[    0,   242,   152,   558,   620,    52,  4165, 11565, 18109,     2,\n",
       "              1,     1,     1,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52,  3675,    16,   368, 12637,\n",
       "          40706, 10557,     2,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52, 18173, 10384,  2400,  7943,\n",
       "             72,  1157,  5062, 12261, 12584,     2],\n",
       "         [    0,  1437,    18,  1824,    46,    52,   379, 30820,     5,     2,\n",
       "              1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])},\n",
       " 'x_2': {'input_ids': tensor([[    0,   242,   152,   558,   620,    52,  4165, 11565, 18109,     2,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52,  3675, 22682, 35211,     2,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52, 18173, 20547,  3189,  7943,\n",
       "             72,    16,   152,  1157, 17080,   102,  1872, 11044, 10557,     2],\n",
       "         [    0,   716,    18,  1824,    46,    52,   379, 30820,     5,     2,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'label': tensor([5.0000, 3.8000, 3.8000, 2.6000], dtype=torch.float64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= SentABDL(vi_sts_df, task= rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL)\n",
    "dataloader= DataLoader(data, batch_size= 4, collate_fn= SentABCollate('vinai/phobert-base-v2',\n",
    "                                                                      mode= 'bi_encoder',\n",
    "                                                                      task= rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL,\n",
    "                                                                    ))\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': {'input_ids': tensor([[    0,   242,   152,   558,   620,    52,  4165, 11565, 18109,     2,\n",
       "            242,   152,   558,   620,    52,  4165, 11565, 18109,     2,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52,  3675,    16,   368, 12637,\n",
       "          40706, 10557,     2,   242,    18,  1824,    46,    52,  3675, 22682,\n",
       "          35211,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1],\n",
       "         [    0,   242,    18,  1824,    46,    52, 18173, 10384,  2400,  7943,\n",
       "             72,  1157,  5062, 12261, 12584,     2,   242,    18,  1824,    46,\n",
       "             52, 18173, 20547,  3189,  7943,    72,    16,   152,  1157, 17080,\n",
       "            102,  1872, 11044, 10557,     2],\n",
       "         [    0,  1437,    18,  1824,    46,    52,   379, 30820,     5,     2,\n",
       "            716,    18,  1824,    46,    52,   379, 30820,     5,     2,     1,\n",
       "              1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "              1,     1,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
       " 'label': tensor([5.0000, 3.8000, 3.8000, 2.6000], dtype=torch.float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= SentABDL(vi_sts_df, task= rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL)\n",
    "dataloader= DataLoader(data, batch_size= 4, collate_fn= SentABCollate('vinai/phobert-base-v2',\n",
    "                                                                      mode= 'cross_encoder',\n",
    "                                                                      task= rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL,\n",
    "                                                                    ))\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label_is_other_embedding'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chatbot.constant.EMBEDDING_CONTRASTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zalo_df= pd.read_json('zalo_norepeate_lower_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thủ_tục công_bố hạn_chế giao_thông đường thủy_...</td>\n",
       "      <td>cảnh_báo thông_báo dự_báo vùng nguy_hiểm thiên...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sông_nho quế là một phụ lưu của sông nào</td>\n",
       "      <td>sông ingoda sông_ingoda ( ) là một con sông nằ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vị thiền_sư nổi_tiếng được suy_tôn ông_tổ nghề...</td>\n",
       "      <td>lý_quốc lý_quốc là một xã thuộc huyện hạ_lang ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai là phó bí_thư thành ủy hiện_tại của hà_nội</td>\n",
       "      <td>thanh hằng phạm_thị_thanh_hằng được biết đến v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vị_thần nào có quyền_lực cao nhất trong văn_hó...</td>\n",
       "      <td>zeus_zeús , hay dzeús ( tiếng hy lạp : ζεύς , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87885</th>\n",
       "      <td>mức chi hoa_hồng đại_lý cho các đại_lý xổ_số đ...</td>\n",
       "      <td>ngoài các khoản chi_phí được xác_định là khoản...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87886</th>\n",
       "      <td>nguyên_tắc thực_hiện dân_chủ trong hoạt_động c...</td>\n",
       "      <td>bác_sỹ có chứng_chỉ hành_nghề khám bệnh chữa b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87887</th>\n",
       "      <td>thời_hạn điều_tra lại các vụ tai_nạn lao_động ...</td>\n",
       "      <td>quyền của cơ_quan bảo_hiểm xã_hội kiểm_tra việ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87888</th>\n",
       "      <td>nữ lập_trình_viên đầu_tiên trên thế_giới là ai</td>\n",
       "      <td>ada aurantiaca brassia aurantiaca là một loài ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87889</th>\n",
       "      <td>cộng_sinh công_nghiệp trong khu công_nghiệp là...</td>\n",
       "      <td>thực_hiện quản_lý nhà_nước về công_nghiệp hoạt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87890 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question  \\\n",
       "0      thủ_tục công_bố hạn_chế giao_thông đường thủy_...   \n",
       "1               sông_nho quế là một phụ lưu của sông nào   \n",
       "2      vị thiền_sư nổi_tiếng được suy_tôn ông_tổ nghề...   \n",
       "3          ai là phó bí_thư thành ủy hiện_tại của hà_nội   \n",
       "4      vị_thần nào có quyền_lực cao nhất trong văn_hó...   \n",
       "...                                                  ...   \n",
       "87885  mức chi hoa_hồng đại_lý cho các đại_lý xổ_số đ...   \n",
       "87886  nguyên_tắc thực_hiện dân_chủ trong hoạt_động c...   \n",
       "87887  thời_hạn điều_tra lại các vụ tai_nạn lao_động ...   \n",
       "87888     nữ lập_trình_viên đầu_tiên trên thế_giới là ai   \n",
       "87889  cộng_sinh công_nghiệp trong khu công_nghiệp là...   \n",
       "\n",
       "                                                    text  label  \n",
       "0      cảnh_báo thông_báo dự_báo vùng nguy_hiểm thiên...      0  \n",
       "1      sông ingoda sông_ingoda ( ) là một con sông nằ...      0  \n",
       "2      lý_quốc lý_quốc là một xã thuộc huyện hạ_lang ...      0  \n",
       "3      thanh hằng phạm_thị_thanh_hằng được biết đến v...      0  \n",
       "4      zeus_zeús , hay dzeús ( tiếng hy lạp : ζεύς , ...      1  \n",
       "...                                                  ...    ...  \n",
       "87885  ngoài các khoản chi_phí được xác_định là khoản...      0  \n",
       "87886  bác_sỹ có chứng_chỉ hành_nghề khám bệnh chữa b...      0  \n",
       "87887  quyền của cơ_quan bảo_hiểm xã_hội kiểm_tra việ...      0  \n",
       "87888  ada aurantiaca brassia aurantiaca là một loài ...      0  \n",
       "87889  thực_hiện quản_lý nhà_nước về công_nghiệp hoạt...      0  \n",
       "\n",
       "[87890 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zalo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sent1': {'input_ids': tensor([[    0,   928,   576,   775,   448,   109, 26255,  2465,  2769,    11,\n",
       "            209,    42,  1279,     2],\n",
       "         [    0, 22340,  5551,  9260,     8,    16,  1746,  3230,     7,   490,\n",
       "            142,     2,     1,     1],\n",
       "         [    0,   626, 26176,   555,    11, 42611, 38423,   657,  6758,    80,\n",
       "              2,     1,     1,     1],\n",
       "         [    0,   277,     8,  2185,  7665,   302, 21356,  1231,   874,     7,\n",
       "          44068,  2151,     2,     1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])},\n",
       " 'sent2': {'input_ids': tensor([[    0,  1223,   655,  ...,     1,     1,     1],\n",
       "         [    0,   490,  1384,  ...,     1,     1,     1],\n",
       "         [    0, 39696,  3705,  ..., 29759,     4,     2],\n",
       "         [    0,  2106,  2416,  ...,    51,    44,     2]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " 'label': tensor([0, 0, 0, 0])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= SentABDL(zalo_df, task= rag_chatbot.constant.EMBEDDING_RANKER_NUMERICAL)\n",
    "dataloader= DataLoader(data, batch_size= 4, collate_fn= SentABCollate('vinai/phobert-base-v2',\n",
    "                                                                      mode= 'bi_encoder',\n",
    "                                                                      task= rag_chatbot.constant.EMBEDDING_CONTRASTIVE,\n",
    "                                                                    ))\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet, In-Batch-Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('label_is_pos_neg', 'multi_negatives')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chatbot.constant.EMBEDDING_TRIPLET, rag_chatbot.constant.EMBEDDING_IN_BATCH_NEGATIVES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mc4-vi-ysinh-00000.json', encoding= 'utf-8') as f: \n",
    "    temp= f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= []\n",
    "for i in temp:\n",
    "    try: \n",
    "        data.append(json.loads(i))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_c4= pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## append 3 columns from text column\n",
    "vi_c4['text_1']= vi_c4.text\n",
    "vi_c4['text_2']= vi_c4.text\n",
    "vi_c4['text_3']= vi_c4.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anchor': {'input_ids': tensor([[    0, 64000,  2902,  ...,     4,   122,     2],\n",
       "         [    0,   107,  1596,  ...,  3621,  2662,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'pos': {'input_ids': tensor([[    0, 64000,  2902,  ..., 45247, 16702,     2],\n",
       "         [    0,   107,  1596,  ...,   653,  5116,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'hard_neg_1': {'input_ids': tensor([[    0, 12445,  2902,  ...,     4,   122,     2],\n",
       "         [    0,   107,  1596,  ...,     6,  3621,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'hard_neg_2': {'input_ids': tensor([[    0, 12445,  2902,  ..., 16702,  2902,     2],\n",
       "         [    0,   107,  1596,  ...,   653,  5116,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in-batch-negatives\n",
    "# this case about anchor, pos, multiple hard negatives\n",
    "data= SentABDL(vi_c4, task= 'multi_negatives')\n",
    "dataloader= DataLoader(data, batch_size= 4, collate_fn= SentABCollate('vinai/phobert-base-v2',\n",
    "                                                                      mode= 'bi_encoder',\n",
    "                                                                      task= rag_chatbot.constant.EMBEDDING_IN_BATCH_NEGATIVES,\n",
    "                                                                      augment_func= NoiseDropout()))\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'anchor': {'input_ids': tensor([[    0, 12445,  2902,  ..., 16702,  2902,     2],\n",
       "         [    0,   107,  1596,  ...,  5116,     6,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'pos': {'input_ids': tensor([[    0, 12445,  2902,  ...,  4517,   653,     2],\n",
       "         [    0,   107,  1596,  ...,  3621,  2662,     2],\n",
       "         [    0, 64000,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'neg': {'input_ids': tensor([[    0, 12445,  2902,  ...,    45,  6007,     2],\n",
       "         [    0,   107,  1596,  ...,  5116,     6,     2],\n",
       "         [    0,   217,  4029,  ...,     1,     1,     1],\n",
       "         [    0,  6861, 16570,  ...,     1,     1,     1]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# triplet \n",
    "# this case about anchor, pos, negative\n",
    "data= SentABDL(vi_c4.iloc[:, :3], task= 'multi_negatives')\n",
    "dataloader= DataLoader(data, batch_size= 4, collate_fn= SentABCollate('vinai/phobert-base-v2',\n",
    "                                                                      mode= 'bi_encoder',\n",
    "                                                                      task= rag_chatbot.constant.EMBEDDING_TRIPLET,\n",
    "                                                                      augment_func= NoiseDropout()))\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
